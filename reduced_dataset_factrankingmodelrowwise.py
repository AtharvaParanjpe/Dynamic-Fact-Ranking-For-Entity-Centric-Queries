# -*- coding: utf-8 -*-
"""Reduced Dataset factRankingModelRowWise.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VlkEcNgTLzXAh0vyHQq7hfnPqUZF0GJX
"""

!pip install --upgrade tensorflow
!pip install keras
!pip install pandas
!pip install numpy
# !pip install -q pyyaml h5py

import pandas as pd
import numpy as np
import re
import keras
import numpy as np
import tensorflow as tf
import math

from keras.layers import LSTM, GRU
from keras.layers.embeddings import Embedding
from keras.preprocessing.sequence import pad_sequences
from keras.models import Model
from keras.layers import Input, Dense, Bidirectional
from keras.layers import concatenate,Dropout

from google.colab import drive
drive.mount("/content/drive")

class MyModel(tf.keras.Model):
    def __init__(self, vocab_size):
        super(MyModel, self).__init__()
        self.embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=(512),embeddings_initializer="glorot_normal")
        self.bidirectional_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512, return_sequences=True))
        # self.dropout_layer = tf.keras.layers.Dropout(0.2)
        self.dense_layer = tf.keras.layers.Dense(1, activation="sigmoid")

    def __call__(self, q, p, o):
        # print(q)
        # print(p)
        # print(o)

        queryEmbedding = self.embedding_layer(q)
        predEmbedding = self.embedding_layer(p)
        objEmbedding = self.embedding_layer(o)
        
        # print(queryEmbedding.shape,predEmbedding.shape,objEmbedding.shape)
        
        queryEmbedding = tf.reshape(queryEmbedding, (1, queryEmbedding.shape[0], queryEmbedding.shape[1]))
        predEmbedding = tf.reshape(predEmbedding, (1, predEmbedding.shape[0], predEmbedding.shape[1]))
        objEmbedding = tf.reshape(objEmbedding, (1, objEmbedding.shape[0], objEmbedding.shape[1]))
        
        
        queryEmbedding = self.bidirectional_lstm(queryEmbedding)
        predEmbedding = self.bidirectional_lstm(predEmbedding)
        objEmbedding = self.bidirectional_lstm(objEmbedding)
        # print(queryEmbedding.shape,predEmbedding.shape,objEmbedding.shape)
        
        ## lstm

        queryConcat = tf.concat([queryEmbedding[:,0,:512],queryEmbedding[:,49,512:]],axis=1)
        predConcat = tf.concat([predEmbedding[:,0,:512],predEmbedding[:,49,512:]],axis=1)
        objConcat = tf.concat([objEmbedding[:,0,:512],objEmbedding[:,49,512:]],axis=1)
        # print(queryConcat.shape,predConcat.shape,objConcat.shape)
        
        # queryConcat = tf.repeat(queryConcat, predConcat.shape[0] , axis=0)
        
        customTarget = queryConcat+predConcat           
        
        customTarget=tf.matmul(customTarget,tf.transpose(objConcat))
        
        # print(customTarget.shape)
        return customTarget

maxLengthPadding = 50

def splitCamelCasing(camelCasedWord):
    camelCaseSplit = re.sub('([A-Z][a-z]+)', r' \1', re.sub('([A-Z]+)', r' \1', camelCasedWord)).split()
    return camelCaseSplit
facts = pd.read_excel('/content/drive/My Drive/Independent Study/output.xlsx')
facts = facts.drop(["id", "qid", "imp", "rel", "en_id"], axis=1)

def invertRanks(rankArray):
  maximum = 5 
  for i in range(len(rankArray)):
    if(rankArray[i]==0):
      rankArray[i]=1/maximum
    else:
      rankArray[i]=rankArray[i]/maximum
  return rankArray

ENCODING_DIM = 50
maxRank = 0
y = invertRanks(list(facts['utility'].values))
print(0.9*len(y))

embeddings_dictionary = {}

## Creates an average for the entire wordList as the number of words may vary
def encode_sequence(wordList):
    encodedValue = []  ## dimension of embedding
    for word in wordList:
        encodedValue += [embeddings_dictionary[word]]
    encodedValue += [0] * (maxLengthPadding - len(encodedValue))
    return encodedValue


################################################# Preparing the input vector ############################################


############# 1. The query encoding ###############

x = []
queryWords = []
adjustedPredicates = []
objWords = []

for i in range(len(facts["pred"])):
# for name, group in groupedResults:
    trainingSample = []

    ## name is the query used for grouping
    tempName = re.sub('[^a-zA-Z0-9 \n\.]', '', facts["query"].values[i].strip())
    queryWords += tempName.strip().split(" ")

    ## encoding predicate-objects

    oW = facts["obj"].values[i].strip()
    if ('<' in oW and '>' in oW and 'dbp' in oW):
        oW = oW.split(":")
        oW = oW[1].split(">")
        oW = oW[0].split('_')
    elif ('www' in oW):
        oW = oW.split(".")
    else:
        oW = oW.split(" ")

        ## for removing special characters
    oW = [re.sub('[^a-zA-Z0-9 \n\.]', '', x) for x in oW]
    objWords += oW
    pred = facts["pred"].values[i].split(":")
    pred = pred[1].split(">")
    pred = splitCamelCasing(pred[0])
    pred = [re.sub('[^a-zA-Z0-9 \n\.]', '', x) for x in pred]
    adjustedPredicates += pred

totalWords = queryWords + adjustedPredicates + objWords
totalWords = set(totalWords)
vocab_size = len(totalWords)

for i, x in enumerate(totalWords):
    embeddings_dictionary[x] = i

queryWordEncodings = []
predWordEncodings = []
objWordEncodings = []


# for name, group in groupedResults:
for i in range(len(facts["pred"].values)):
    ## name is the query used for grouping
    name = re.sub('[^a-zA-Z0-9 \n\.]', '', facts["query"].values[i])
    queryWords = name.strip().split(" ")
    queryWordEncodings += [encode_sequence(queryWords)]
    ## encoding predicate-objects
    oW = facts["obj"].values[i].strip()
    if ('<' in oW and '>' in oW and 'dbp' in oW):
        oW = oW.split(":")
        oW = oW[1].split(">")
        oW = oW[0].split('_')
    elif ('www' in oW):
        oW = oW.split(".")
    else:
        oW = oW.split(" ")

    ## for removing special characters
    oW = [re.sub('[^a-zA-Z0-9 \n\.]', '', x) for x in oW]
    oW = encode_sequence(oW)
    objWordEncodings += [oW]

    


    pred = facts["pred"].values[i].split(":")
    pred = pred[1].split(">")
    pred = splitCamelCasing(pred[0])
    pred = [re.sub('[^a-zA-Z0-9 \n\.]', '', x) for x in pred]
    predWordEncodings += [encode_sequence(pred)]

print(len(queryWordEncodings[0]),len(objWordEncodings[0]),len(predWordEncodings[0]))

model = MyModel(vocab_size)
loss_obj = tf.keras.losses.MeanSquaredError()
optimizer = tf.keras.optimizers.Adam()

train_loss = tf.keras.metrics.Mean(name='train_loss')
train_accuracy = tf.keras.metrics.MeanSquaredError(name='train_accuracy')

test_loss = tf.keras.metrics.Mean(name='test_loss')
test_accuracy = tf.keras.metrics.MeanSquaredError(name='test_accuracy')

# print(len(queryWordEncodings))

def train_step(x_tr,y_tr):
  qE = x_tr[0]
  pE = x_tr[1]
  oE = x_tr[2]
  loss_mean_training = 0
  for i in range(len(qE)):
      with tf.GradientTape() as tape:
          q= tf.convert_to_tensor(qE[i], dtype=tf.int32)
          p = tf.convert_to_tensor(pE[i], dtype=tf.int32)
          o = tf.convert_to_tensor(oE[i], dtype=tf.int32)
          print(q,p,o)
          predictions = model(q, p, o)
          loss = loss_obj(y_tr[i],predictions)
          loss_mean_training+=loss
      gradients = tape.gradient(loss, model.trainable_variables)
      optimizer.apply_gradients(zip(gradients, model.trainable_variables))
  return loss_mean_training/len(qE)
  
  

def test_step(x_ts,y_ts):
    queryWordEncodings = x_ts[0]
    predWordEncodings = x_ts[1]
    objWordEncodings = x_ts[2]
    loss_mean_test = 0
    # print("--------------------------------------------------------x--------------------------------------------------------x-----------------------------------------------")  
    for i in range(len(queryWordEncodings)):
        q= tf.convert_to_tensor(queryWordEncodings[i], dtype=tf.int32)
        p = tf.convert_to_tensor(predWordEncodings[i], dtype=tf.int32)
        o = tf.convert_to_tensor(objWordEncodings[i], dtype=tf.int32)
        predictions = model(q,p,o)
        t_loss = loss_obj(y_ts[i], predictions)
        loss_mean_test+=t_loss
        test_loss(t_loss)
        test_accuracy(y_test[i], predictions)
        # print(i)
    return loss_mean_test/len(queryWordEncodings)

from sklearn.utils import shuffle

EPOCHS = 15

data_len = int(0.9*len(queryWordEncodings))

queryWordEncodingsTrain,predWordEncodingsTrain,objWordEncodingsTrain,y_train = shuffle(queryWordEncodings[:data_len],objWordEncodings[:data_len],predWordEncodings[:data_len],y[:data_len])
queryWordEncodingsTest,predWordEncodingsTest,objWordEncodingsTest,y_test = shuffle(queryWordEncodings[data_len:],objWordEncodings[data_len:],predWordEncodings[data_len:],y[data_len:])



batch_size = 16

num_batches_train = math.floor(data_len/batch_size)

test_size = len(queryWordEncodings) - data_len
num_batches_test = math.floor(test_size/batch_size)
print("Num Batches Train:",num_batches_train)
print(data_len/batch_size)
print(len(y_train))

for epoch in range(EPOCHS):
  # Reset the metrics at the start of the next epoch
  train_loss.reset_states()
  train_accuracy.reset_states()
  test_loss.reset_states()
  test_accuracy.reset_states()

  for i in range(num_batches_train):
    x_train = [queryWordEncodingsTrain[i*batch_size:(i+1)*batch_size],predWordEncodingsTrain[i*batch_size:(i+1)*batch_size],objWordEncodingsTrain[i*batch_size:(i+1)*batch_size]]
    loss1 = train_step(x_train,y_train[i*batch_size:(i+1)*batch_size])
    print(loss1)
  x_train = [queryWordEncodingsTrain[num_batches_train*batch_size:],predWordEncodingsTrain[num_batches_train*batch_size:],objWordEncodingsTrain[num_batches_train*batch_size:]]
  loss1 = train_step(x_train,y_train[num_batches_train*batch_size:])
  print(loss1)

  for i in range(num_batches_test):
    x_test = [queryWordEncodingsTest[i*batch_size:(i+1)*batch_size],predWordEncodingsTest[i*batch_size:(i+1)*batch_size],objWordEncodingsTest[i*batch_size:(i+1)*batch_size]]
    loss2 = test_step(x_test,y_test[i*batch_size:(i+1)*batch_size])
    print(loss2)  

  x_test = [queryWordEncodingsTest[num_batches_test*batch_size:],predWordEncodingsTest[num_batches_test*batch_size:],objWordEncodingsTest[num_batches_test*batch_size:]]
  loss2 = test_step(x_test,y_test[num_batches_test*batch_size:])
  print(loss2)

  print("---------------------------------------------------------x---------------------------------------------x----------------------------------------------------------")
  # template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'
  # print(template.format(epoch+1,
  #                       train_loss.result(),
  #                       train_accuracy.result()*100,
  #                       test_loss.result(),
  #                       test_accuracy.result()*100))

# queryWordEncodings = x_train[0]
# predWordEncodings = x_train[1]
# objWordEncodings = x_train[2]
# for i in range(len(queryWordEncodings)):
#   q= tf.convert_to_tensor(queryWordEncodings[i], dtype=tf.int32)
#   p = tf.convert_to_tensor(predWordEncodings[i], dtype=tf.int32)
#   o = tf.convert_to_tensor(objWordEncodings[i], dtype=tf.int32)
#   predictions = model(q, p, o)
#   print(y_train[i])
#   print(predictions)
#   input()

## First Model with no randomization , no data modification
# model.save_weights('/content/drive/My Drive/Independent Study/SavedModel') 

model.save_weights('/content/drive/My Drive/Independent Study/URI_Data')

model_load = MyModel(vocab_size)
model_load.load_weights('/content/drive/My Drive/Independent Study/URI_Data')

# queryWordEncodings = queryWordEncodings[:50]
# predWordEncodings = predWordEncodings[:50]
# objWordEncodings = objWordEncodings[:50]

# predictedQueryRanks = []
# groundTruthRanks = []
# for i in range(len(queryWordEncodings)):
#   q= tf.convert_to_tensor(queryWordEncodings[i], dtype=tf.int32)
#   p = tf.convert_to_tensor(predWordEncodings[i], dtype=tf.int32)
#   o = tf.convert_to_tensor(objWordEncodings[i], dtype=tf.int32)
#   predictions = model_load(q, p, o)
#   print(predictions*5)
#   predictedQueryRanks.append(tf.math.round(predictions*5).numpy().tolist()[0][0])
#   groundTruthRanks.append(y[i]*5) 

# print(predictedQueryRanks[0:10])
# print(groundTruthRanks[0:10])

# print("-----------------------x----------------------------x-------------------")
# print(predictedQueryRanks[10:20])
# print(groundTruthRanks[10:20])

# print("-----------------------x----------------------------x-------------------")

# print(predictedQueryRanks[20:30])
# print(groundTruthRanks[20:30])

# print("-----------------------x----------------------------x-------------------")

# print(predictedQueryRanks[30:40])
# print(groundTruthRanks[30:40])

groupedResults = facts.groupby('query')

def encode_sequence(wordList):
    encodedValue = []  ## dimension of embedding
    for word in wordList:
        encodedValue += [embeddings_dictionary[word]]
    encodedValue += [0] * (maxLengthPadding - len(encodedValue))
    return encodedValue


def modifiedInvertRanks(rankArray):
  for rank in rankArray:
    maximum = 5 
    for j in range(len(rank)):
      if(rank[j]==0):
        rank[j]=1/maximum
      else:
        rank[j]=rank[j]/maximum
    
  return rankArray

modified_y = []
groupedResults = facts.groupby('query')

for name, group in groupedResults:
    g = list(group['utility'].values)
    # print(len(g))
    modified_y.append(g)
modified_y = modifiedInvertRanks(modified_y)


queryWordEncodings = []
predWordEncodings = []
objWordEncodings = []

predictedQueryRanks = []
groundTruthRanks = []

count = 0
# print("--------------------------x---------------------------x----------------x--------------")
for name, group in groupedResults:
    ## name is the query used for grouping
    name = re.sub('[^a-zA-Z0-9 \n\.]', '', name)
    queryWords = name.strip().split(" ")
    q = encode_sequence(queryWords)
    predictedRanksPerQuery = []
    groundTruthRanksPerQuery = []
    ## encoding predicate-objects
    print(len(group["obj"].values))
    # print(len(group["utility"].values))
    for i in range(len(group["obj"].values)):
        oW = group["obj"].values[i].strip()
        if ('<' in oW and '>' in oW and 'dbp' in oW):
            oW = oW.split(":")
            oW = oW[1].split(">")
            oW = oW[0].split('_')
        elif ('www' in oW):
            oW = oW.split(".")
        else:
            oW = oW.split(" ")

        ## for removing special characters
        oW = [re.sub('[^a-zA-Z0-9 \n\.]', '', x) for x in oW]
        o = encode_sequence(oW)
        
        pred = group["pred"].values[i].split(":")
        pred = pred[1].split(">")
        pred = splitCamelCasing(pred[0])
        pred = [re.sub('[^a-zA-Z0-9 \n\.]', '', x) for x in pred]
        p = encode_sequence(pred)
        
        q= tf.convert_to_tensor(q, dtype=tf.int32)
        p = tf.convert_to_tensor(p, dtype=tf.int32)
        o = tf.convert_to_tensor(o, dtype=tf.int32)
        
        predictions = model_load(q, p, o)
        # print(predictions*5)
        predictedRanksPerQuery.append(tf.math.round(predictions*5).numpy().tolist()[0][0])

    g_t = [x*5 for x in modified_y[count]]
    groundTruthRanks.append(g_t)
    predictedQueryRanks.append(predictedRanksPerQuery)
    count+=1

print(len(groundTruthRanks[0]),len(groundTruthRanks[1]),len(groundTruthRanks[2]))
print(len(predictedQueryRanks[0]),len(predictedQueryRanks[1]),len(predictedQueryRanks[2]))

def dcg_score(y_true, y_score, k=5):
    order = np.argsort(y_score)[::-1]
    y_true = np.take(y_true, order[:k])
    gain = 2 ** y_true - 1
    discounts = np.log2(np.arange(len(y_true)) + 2)
    return np.sum(gain / discounts)


def ndcg_score(ground_truth, predictions, k=5):
    scores = []
    for y_true, y_score in zip(ground_truth, predictions):
        actual = dcg_score(y_true, y_score, k)
        best = dcg_score(y_true, y_true, k)
        score = float(actual) / float(best)
        scores.append(score)


    return np.mean(scores)

print(ndcg_score(groundTruthRanks,predictedQueryRanks,k=5))

!pip install sklearn

from sklearn.metrics import dcg_score

score_list = []
for i in range(len(groundTruthRanks)):
  print(groundTruthRanks[i])
  print(predictedQueryRanks[i])
  s1 = dcg_score(groundTruthRanks[i],predictedQueryRanks[i],k=5) 
  s2 = dcg_score(groundTruthRanks[i],groundTruthRanks[i],k=5)
  score_list.append(s1/s2)
  print("--------------------------------------------x------------------------------------------------------x----------------------------------------")
print(sum(score_list)/len(score_list))


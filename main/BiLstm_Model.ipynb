{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BiLstm Model",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "slCrtu6DptEj",
        "colab_type": "code",
        "outputId": "517dc359-076e-4bc7-fb07-2fa3e53332a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        }
      },
      "source": [
        "!pip install --upgrade tensorflow\n",
        "!pip install keras\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "# !pip install -q pyyaml h5py\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import keras\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import math\n",
        "\n",
        "from keras.layers import LSTM, GRU\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Bidirectional\n",
        "from keras.layers import concatenate,Dropout"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorflow in /usr/local/lib/python3.6/dist-packages (2.2.0rc4)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.3)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied, skipping upgrade: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.2.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.28.1)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (46.1.3)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.7.2)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.6.0.post3)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2020.4.5.1)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.9)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.3.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qm5Pw8lRrdF_",
        "colab_type": "code",
        "outputId": "ac9c6091-40ad-4209-da38-0570b5338b25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14DZjA9qq2P_",
        "colab_type": "text"
      },
      "source": [
        "Given a natural language query $q$ and a set of (predicate, object) tuples pertaining to an entity, the model aims to predict the *importance* and *utility* ranks of the (predicate, object) tuples with respect to the query $q$.\n",
        "\n",
        "We encode the query $q$ and the predicate $p$ into $d$-dimensional embedding vectors $h_q$ and $h_p$ using a BiLSTM. Then, the model predicts a score for by using a dot product of $h_q$ and $h_p$. i.e. $\\Psi(q, p) = h_q^Th_p$\n",
        "\n",
        "We minimize the MSE Loss between the predicted score and the target utility or importance score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-48b_J8Djeo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel2(tf.keras.Model):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(MyModel2, self).__init__()\n",
        "        self.embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=(512),embeddings_initializer=\"glorot_normal\",mask_zero=True)\n",
        "        self.bidirectional_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512, return_sequences=True))\n",
        "        self.dense_layer = tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "\n",
        "    def __call__(self, q, p, o):\n",
        "\n",
        "        ## embedding layer\n",
        "        q_token_embeddings = self.embedding_layer(q)\n",
        "        p_token_embeddings = self.embedding_layer(p)\n",
        "        o_token_embeddings = self.embedding_layer(o)        \n",
        "\n",
        "        ## reshaping before feeding to Bi-LSTM\n",
        "        q_token_embeddings = tf.reshape(q_token_embeddings, (1, q_token_embeddings.shape[0], q_token_embeddings.shape[1]))\n",
        "        p_token_embeddings = tf.reshape(p_token_embeddings, (1, p_token_embeddings.shape[0], p_token_embeddings.shape[1]))\n",
        "        o_token_embeddings = tf.reshape(o_token_embeddings, (1, o_token_embeddings.shape[0], o_token_embeddings.shape[1]))\n",
        "        \n",
        "        ## Bi-LSTM\n",
        "        queryEmbedding = self.bidirectional_lstm(q_token_embeddings)\n",
        "        predEmbedding = self.bidirectional_lstm(p_token_embeddings)\n",
        "        objEmbedding = self.bidirectional_lstm(o_token_embeddings)\n",
        "        \n",
        "        ## taking the first and last outputs of lstm layer \n",
        "        ## first -> right context\n",
        "        ## last -> left context\n",
        "        query_emb = tf.concat([queryEmbedding[:,0,:512],queryEmbedding[:,49,512:]],axis=1)\n",
        "        pred_emb = tf.concat([predEmbedding[:,0,:512],predEmbedding[:,49,512:]],axis=1)\n",
        "        obj_emb = tf.concat([objEmbedding[:,0,:512],objEmbedding[:,49,512:]],axis=1)\n",
        "\n",
        "        ## q.pT  \n",
        "        scores = tf.matmul(query_emb, tf.transpose(pred_emb))\n",
        "        # print(scores.shape)\n",
        "        \n",
        "        return scores\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tQvFmHdRRjs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## homogeneous encoding for all the queries,predicates, objects\n",
        "maxLengthPadding = 50\n",
        "\n",
        "\n",
        "## split the words in predicates and objects\n",
        "def splitCamelCasing(camelCasedWord):\n",
        "    camelCaseSplit = re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1', camelCasedWord)).split()\n",
        "    return camelCaseSplit\n",
        "\n",
        "## different datasets\n",
        "\n",
        "# facts = pd.read_excel('/content/drive/My Drive/Independent Study/output.xlsx')\n",
        "facts = pd.read_csv('/content/drive/My Drive/Independent Study/fact_ranking_coll.tsv', delimiter='\\t', encoding='utf-8')\n",
        "\n",
        "\n",
        "facts = facts.drop([\"id\", \"qid\", \"rel\", \"en_id\"], axis=1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GIFg_XARqJU",
        "colab_type": "code",
        "outputId": "3fb91555-7d4c-4b7c-904f-62c0eceb723e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "## To give a higher importance to a higher utility value\n",
        "\n",
        "def invertRanks(rankArray):\n",
        "  maximum = 2 \n",
        "  for i in range(len(rankArray)):\n",
        "    rankArray[i]=rankArray[i]/maximum\n",
        "  return rankArray\n",
        "\n",
        "\n",
        "ENCODING_DIM = 50\n",
        "maxRank = 0\n",
        "y = invertRanks(list(facts['imp'].values))\n",
        "print(y[10:100])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.5, 0.5, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.5, 1.0, 0.0, 0.5, 0.0, 0.5, 0.5, 0.5, 1.0, 0.0, 0.5, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rV-xh92WqapC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings_dictionary = {}\n",
        "\n",
        "## Creates an encoding for the entire wordList\n",
        "def encode_sequence(wordList):\n",
        "    encodedValue = []  ## dimension of embedding\n",
        "    for word in wordList:\n",
        "        encodedValue += [embeddings_dictionary[word]]\n",
        "    ## padding the remaining values to create a homogeneous encoding\n",
        "    encodedValue += [0] * (maxLengthPadding - len(encodedValue))\n",
        "    return encodedValue\n",
        "\n",
        "\n",
        "################################################# Preparing the Embedding Space / Dictionary of Words ############################################\n",
        "\n",
        "\n",
        "############# 1. Preparing the embedding layer ###############\n",
        "\n",
        "x = []\n",
        "queryWords = []\n",
        "adjustedPredicates = []\n",
        "objWords = []\n",
        "\n",
        "for i in range(len(facts[\"pred\"])):\n",
        "    ## splitting the query\n",
        "    tempName = re.sub('[^a-zA-Z0-9 \\n\\.]', '', facts[\"query\"].values[i].strip())\n",
        "    queryWords += tempName.lower().strip().split(\" \")\n",
        "\n",
        "    ## splitting the object\n",
        "    oW = facts[\"obj\"].values[i].lower().strip()\n",
        "    if ('<' in oW and '>' in oW and 'dbp' in oW):\n",
        "        oW = oW.split(\":\")\n",
        "        oW = oW[1].split(\">\")\n",
        "        oW = oW[0].split('_')\n",
        "    elif ('www' in oW):\n",
        "        oW = oW.split(\".\")\n",
        "    else:\n",
        "        oW = oW.split(\" \")\n",
        "\n",
        "    ## for removing special characters\n",
        "    oW = [re.sub('[^a-zA-Z0-9 \\n\\.]', '', x) for x in oW]\n",
        "    objWords += oW\n",
        "    \n",
        "    #splitting the predicates\n",
        "    pred = facts[\"pred\"].values[i].lower().split(\":\")\n",
        "    pred = pred[1].split(\">\")\n",
        "    pred = splitCamelCasing(pred[0])\n",
        "    pred = [re.sub('[^a-zA-Z0-9 \\n\\.]', '', x) for x in pred]\n",
        "    adjustedPredicates += pred\n",
        "\n",
        "totalWords = queryWords + adjustedPredicates + objWords\n",
        "totalWords = set(totalWords)\n",
        "vocab_size = len(totalWords)\n",
        "\n",
        "## creating the dictionary\n",
        "for i, x in enumerate(totalWords):\n",
        "    embeddings_dictionary[x] = i\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4wocCFARiYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### embedding the tokens\n",
        "\n",
        "queryWordEncodings = []\n",
        "predWordEncodings = []\n",
        "objWordEncodings = []\n",
        "\n",
        "\n",
        "for i in range(len(facts[\"pred\"].values)):\n",
        "\n",
        "    name = re.sub('[^a-zA-Z0-9 \\n\\.]', '', facts[\"query\"].values[i])\n",
        "    queryWords = name.lower().strip().split(\" \")\n",
        "    queryWordEncodings += [encode_sequence(queryWords)]\n",
        "    \n",
        "    ## similar to creating the dictionary, but this time it encodes the words using the values from dictionary\n",
        "    oW = facts[\"obj\"].values[i].lower().strip()\n",
        "    if ('<' in oW and '>' in oW and 'dbp' in oW):\n",
        "        oW = oW.split(\":\")\n",
        "        oW = oW[1].split(\">\")\n",
        "        oW = oW[0].split('_')\n",
        "    elif ('www' in oW):\n",
        "        oW = oW.split(\".\")\n",
        "    else:\n",
        "        oW = oW.split(\" \")\n",
        "\n",
        "    ## for removing special characters\n",
        "    oW = [re.sub('[^a-zA-Z0-9 \\n\\.]', '', x) for x in oW]\n",
        "    oW = encode_sequence(oW)\n",
        "    objWordEncodings += [oW]\n",
        "\n",
        "    ## encoding the words of predicates\n",
        "    pred = facts[\"pred\"].values[i].lower().split(\":\")\n",
        "    pred = pred[1].split(\">\")\n",
        "    pred = splitCamelCasing(pred[0])\n",
        "    pred = [re.sub('[^a-zA-Z0-9 \\n\\.]', '', x) for x in pred]\n",
        "    predWordEncodings += [encode_sequence(pred)]\n",
        "\n",
        "# print(len(queryWordEncodings[0]),len(objWordEncodings[0]),len(predWordEncodings[0]))\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3plOo820z0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## defining custom model parameters as per tensorflow2 guidelines\n",
        "\n",
        "model_2 = MyModel2(vocab_size)\n",
        "loss_obj = tf.keras.losses.MeanSquaredError()\n",
        "optimizer = tf.keras.optimizers.Adam( learning_rate=0.0001)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.MeanSquaredError(name='train_accuracy')\n",
        "\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.MeanSquaredError(name='test_accuracy')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIdv_M87dRwi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## x_tr : training x -> contains the query data, predicate data, object data\n",
        "## y_tr : training y -> contains the normalized targets\n",
        "def train_step(x_tr,y_tr):\n",
        "    qE = x_tr[0]\n",
        "    pE = x_tr[1]\n",
        "    oE = x_tr[2]\n",
        "    batch_loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        for i in range(len(qE)):\n",
        "            q= tf.convert_to_tensor(qE[i], dtype=tf.int32)\n",
        "            p = tf.convert_to_tensor(pE[i], dtype=tf.int32)\n",
        "            o = tf.convert_to_tensor(oE[i], dtype=tf.int32)\n",
        "            predictions = model_2(q, p, o)\n",
        "            loss = loss_obj(y_tr[i],predictions)\n",
        "            batch_loss+=loss\n",
        "        gradients = tape.gradient(batch_loss/len(qE), model_2.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model_2.trainable_variables))\n",
        "    return batch_loss\n",
        "  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUsDyeeWiXo3",
        "colab_type": "code",
        "outputId": "864547a1-737e-4fae-c5c9-82c07c977d7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "## train test split step\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "EPOCHS = 10\n",
        "\n",
        "data_len = int(0.8*len(queryWordEncodings))\n",
        "# data_len = len(queryWordEncodings)\n",
        "queryWordEncodingsTrain,predWordEncodingsTrain,objWordEncodingsTrain,y_train = shuffle(queryWordEncodings[:data_len],objWordEncodings[:data_len],predWordEncodings[:data_len],y[:data_len])\n",
        "queryWordEncodingsTest,predWordEncodingsTest,objWordEncodingsTest,y_test = shuffle(queryWordEncodings[data_len:],objWordEncodings[data_len:],predWordEncodings[data_len:],y[data_len:])\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "num_batches_train = math.floor(data_len/batch_size)\n",
        "\n",
        "test_size = len(queryWordEncodings)\n",
        "num_batches_test = math.floor(test_size/batch_size)\n",
        "\n",
        "\n",
        "print(\"Num Batches Train:\",num_batches_train)\n",
        "print(data_len/batch_size)\n",
        "print(len(y_train))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num Batches Train: 203\n",
            "203.4375\n",
            "3255\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcgg3mzGXS65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ###### To check the number of predicates not appearing in the train set\n",
        "# count = 0\n",
        "\n",
        "# for p in predWordEncodingsTest:\n",
        "#   if(p in pWTrainSet):\n",
        "#     count+=1\n",
        "#     pWTrainSet.append(p)\n",
        "# print(count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcF6r-mNauYz",
        "colab_type": "code",
        "outputId": "3d7c2ecd-041b-4fa0-f9d1-42957c154b12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "## main epoch analysis\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  # Reset the metrics at the start of the next epoch\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  test_accuracy.reset_states()\n",
        "\n",
        "  loss_per_epoch = 0\n",
        "\n",
        "  for i in range(num_batches_train):\n",
        "    x_train = [queryWordEncodingsTrain[i*batch_size:(i+1)*batch_size],predWordEncodingsTrain[i*batch_size:(i+1)*batch_size],objWordEncodingsTrain[i*batch_size:(i+1)*batch_size]]\n",
        "    loss1 = train_step(x_train,y_train[i*batch_size:(i+1)*batch_size])\n",
        "    loss_per_epoch += loss1\n",
        "  x_train = [queryWordEncodingsTrain[num_batches_train*batch_size:],predWordEncodingsTrain[num_batches_train*batch_size:],objWordEncodingsTrain[num_batches_train*batch_size:]]\n",
        "  loss1 = train_step(x_train,y_train[num_batches_train*batch_size:])\n",
        "  loss_per_epoch += loss1\n",
        "  print(\"Epoch Loss for epoch \"+str(epoch)+\": \",loss_per_epoch/(num_batches_train+1) )\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch Loss for epoch 0:  tf.Tensor(1.927495, shape=(), dtype=float32)\n",
            "Epoch Loss for epoch 1:  tf.Tensor(1.035517, shape=(), dtype=float32)\n",
            "Epoch Loss for epoch 2:  tf.Tensor(0.69604474, shape=(), dtype=float32)\n",
            "Epoch Loss for epoch 3:  tf.Tensor(0.5969088, shape=(), dtype=float32)\n",
            "Epoch Loss for epoch 4:  tf.Tensor(0.550104, shape=(), dtype=float32)\n",
            "Epoch Loss for epoch 5:  tf.Tensor(0.49941587, shape=(), dtype=float32)\n",
            "Epoch Loss for epoch 6:  tf.Tensor(0.48625115, shape=(), dtype=float32)\n",
            "Epoch Loss for epoch 7:  tf.Tensor(0.47115016, shape=(), dtype=float32)\n",
            "Epoch Loss for epoch 8:  tf.Tensor(0.4643826, shape=(), dtype=float32)\n",
            "Epoch Loss for epoch 9:  tf.Tensor(0.41294488, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETUejsMdbRpX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model_2.save_weights('/content/drive/My Drive/Independent Study/MiniBatch_URI_Data')\n",
        "# # model_2.save_weights('/content/drive/My Drive/Independent Study/MiniBatch_Entire_Data_Imp')\n",
        "# model_load = MyModel2(vocab_size)\n",
        "# model_load.load_weights('/content/drive/My Drive/Independent Study/MiniBatch_URI_Data')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW5-9wsnm_7M",
        "colab_type": "code",
        "outputId": "559674f6-a417-4f51-d319-d02e636d26cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "## grouping the dataset by query to get the ndcg scores\n",
        "\n",
        "def modifiedInvertRanks(rankArray):\n",
        "  for rank in rankArray:\n",
        "    maximum = 2 \n",
        "    for j in range(len(rank)):\n",
        "      rank[j]=rank[j]/maximum\n",
        "    \n",
        "  return rankArray\n",
        "\n",
        "modified_y = []\n",
        "groupedResults = facts.groupby('query')\n",
        "\n",
        "for name, group in groupedResults:\n",
        "    g = list(group['imp'].values)\n",
        "    # print(len(g))\n",
        "    modified_y.append(g)\n",
        "modified_y = modifiedInvertRanks(modified_y)\n",
        "\n",
        "\n",
        "queryWordEncodings = []\n",
        "predWordEncodings = []\n",
        "objWordEncodings = []\n",
        "\n",
        "predictedQueryRanks = []\n",
        "groundTruthRanks = []\n",
        "integerValuedQueryRanks = []\n",
        "\n",
        "count = 0\n",
        "\n",
        "for query, group in groupedResults:\n",
        "\n",
        "    ## name is the query used for grouping\n",
        "    query = re.sub('[^a-zA-Z0-9 \\n\\.]', '', query)\n",
        "    queryWords = query.lower().strip().split(\" \")\n",
        "    q = encode_sequence(queryWords)\n",
        "    predictedRanksPerQuery = []\n",
        "    groundTruthRanksPerQuery = []\n",
        "\n",
        "    ## encoding predicate-objects\n",
        "    i_ranks = []\n",
        "    for i in range(len(group[\"obj\"].values)):\n",
        "        oW = group[\"obj\"].values[i].lower().strip()\n",
        "        if ('<' in oW and '>' in oW and 'dbp' in oW):\n",
        "            oW = oW.split(\":\")\n",
        "            oW = oW[1].split(\">\")\n",
        "            oW = oW[0].split('_')\n",
        "        elif ('www' in oW):\n",
        "            oW = oW.split(\".\")\n",
        "        else:\n",
        "            oW = oW.split(\" \")\n",
        "\n",
        "        ## for removing special characters\n",
        "        oW = [re.sub('[^a-zA-Z0-9 \\n\\.]', '', x) for x in oW]\n",
        "        o = encode_sequence(oW)\n",
        "        \n",
        "        pred = group[\"pred\"].values[i].lower().split(\":\")\n",
        "        pred = pred[1].split(\">\")\n",
        "        pred = splitCamelCasing(pred[0])\n",
        "        pred = [re.sub('[^a-zA-Z0-9 \\n\\.]', '', x) for x in pred]\n",
        "        p = encode_sequence(pred)\n",
        "        \n",
        "        q= tf.convert_to_tensor(q, dtype=tf.int32)\n",
        "        p = tf.convert_to_tensor(p, dtype=tf.int32)\n",
        "        o = tf.convert_to_tensor(o, dtype=tf.int32)\n",
        "        \n",
        "        predictions = model_2(q, p, o)\n",
        "        predictedRanksPerQuery.append(predictions.numpy().tolist()[0][0])\n",
        "        i_ranks.append(round(predictions.numpy().tolist()[0][0]*2))\n",
        "    # i_ranks = [x if x < 4 else 4 for x in i_ranks]\n",
        "    integerValuedQueryRanks.append(i_ranks)\n",
        "\n",
        "    g_t = [x*2 for x in modified_y[count]]\n",
        "    groundTruthRanks.append(g_t)\n",
        "    predictedQueryRanks.append(predictedRanksPerQuery)\n",
        "    count+=1\n",
        "\n",
        "print(groundTruthRanks[0])\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.0, 0.0, 0.0, 1.0, 2.0, 1.0, 1.0, 0.0, 0.0, 2.0, 1.0, 1.0, 2.0, 1.0, 2.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWVmzFFjO8NX",
        "colab_type": "code",
        "outputId": "9f9c49fb-d2b2-490e-b39e-1db81967330e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "## NDCG implementation\n",
        "from sklearn.metrics import ndcg_score, dcg_score \n",
        " \n",
        "ndcg_scores_5 = []\n",
        "ndcg_scores_10 = []\n",
        "count = 0\n",
        "for x,y in zip(groundTruthRanks,integerValuedQueryRanks):\n",
        "    \n",
        "    \n",
        "    if(len(x)>1):\n",
        "        true_relevance = np.asarray([x]) \n",
        "        relevance_score = np.asarray([y]) \n",
        "\n",
        "        ndcg_scores_5.append(ndcg_score(true_relevance, relevance_score,k=5))\n",
        "        ndcg_scores_10.append(ndcg_score(true_relevance, relevance_score,k=10))\n",
        "    \n",
        "\n",
        "print(sum(ndcg_scores_5)/len(ndcg_scores_5))\n",
        "print(sum(ndcg_scores_10)/len(ndcg_scores_10))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5172612840243712\n",
            "0.5622571771285199\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIMqj4ss4aM3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
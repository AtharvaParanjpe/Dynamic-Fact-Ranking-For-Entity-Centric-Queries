{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert Without Fine Tuning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "slCrtu6DptEj",
        "colab_type": "code",
        "outputId": "821fa2d0-b234-480d-ee05-a139879de5c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install --upgrade tensorflow\n",
        "!pip install transformers\n",
        "!pip install keras\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "# !pip install -q pyyaml h5py\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import keras\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import math\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# from keras.layers import LSTM, GRU\n",
        "# from keras.layers.embeddings import Embedding\n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "# from keras.models import Model\n",
        "from keras.layers import Input, Dense, Bidirectional,Flatten\n",
        "# from keras.layers import concatenate,Dropout\n",
        "from transformers import BertTokenizer, TFBertModel,TFBertForSequenceClassification"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorflow in /usr/local/lib/python3.6/dist-packages (2.2.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.29.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n",
            "Requirement already satisfied, skipping upgrade: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.4)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.2.1)\n",
            "Requirement already satisfied, skipping upgrade: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.6.0.post3)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (46.3.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.7.2)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.6.0)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.9)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2020.4.5.1)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.8)\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/b5/ac41e3e95205ebf53439e4dd087c58e9fd371fd8e3724f2b9b4cdb8282e5/transformers-2.10.0-py3-none-any.whl (660kB)\n",
            "\u001b[K     |████████████████████████████████| 665kB 4.7MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 46.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 47.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 60.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=5e81f675ecc8967fe7ecab7a41996583ade4afadd1bbb7facf63c86c3d49aa28\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.10.0\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.3.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.4)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qm5Pw8lRrdF_",
        "colab_type": "code",
        "outputId": "c32406cc-78ed-4367-9dc7-7c4504eaac12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14DZjA9qq2P_",
        "colab_type": "text"
      },
      "source": [
        "Given a natural language query $q$ and a set of (predicate, object) tuples pertaining to an entity, the model aims to predict the *importance* and *utility* ranks of the (predicate, object) tuples with respect to the query $q$.\n",
        "\n",
        "We encode the query $q$ and the predicate $p$ into $d$-dimensional embedding vectors $h_q$ and $h_p$ using a BiLSTM. Then, the model predicts a score for by using a dot product of $h_q$ and $h_p$. i.e. $\\Psi(q, p) = h_q^Th_p$\n",
        "\n",
        "We minimize the MSE Loss between the predicted score and the target utility or importance score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-48b_J8Djeo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        # self.bert_layer = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dense_layer = tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "        ## 50 x 768\n",
        "\n",
        "    def __call__(self, x_train):\n",
        "        # intermediate = self.bert_layer(x_train)\n",
        "        output = self.dense_layer(x_train[1])\n",
        "        \n",
        "        \n",
        "        # input()\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fitav0Tu1ot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## homogeneous encoding for all the queries,predicates, objects\n",
        "maxLengthPadding = 50\n",
        "# Note : maxLengthOfTokens = 48;\n",
        "\n",
        "input_dictionary = {}\n",
        "\n",
        "# df = pd.read_csv(\"/content/drive/My Drive/Independent Study/Modified_Data.csv\")\n",
        "# df = pd.read_csv(\"/content/drive/My Drive/Independent Study/URI_only.csv\")\n",
        "\n",
        "\n",
        "## final datasets\n",
        "# df = pd.read_csv(\"/content/drive/My Drive/Independent Study/Complete_Data.csv\")\n",
        "df = pd.read_csv(\"/content/drive/My Drive/Independent Study/URI_with_imp.csv\")\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "## create the position id and segment id\n",
        "\n",
        "def getBertParameters(tokens):\n",
        "    attn_mask = []\n",
        "    seg_ids = []\n",
        "    # pos_ids = []\n",
        "    if(len(tokens)<maxLengthPadding):\n",
        "        attn_mask = [1]*len(tokens) + [0]*(maxLengthPadding-len(tokens))\n",
        "    else:\n",
        "        attn_mask = [1]*maxLengthPadding\n",
        "    \n",
        "    segment = 0\n",
        "    for x in tokens:\n",
        "        seg_ids.append(segment)\n",
        "        if(x=='[SEP]'):\n",
        "            segment = 1\n",
        "    seg_ids+=[0]*(maxLengthPadding-len(tokens))\n",
        "    return attn_mask,seg_ids\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Py8T5xBrV93V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## defining custom model parameters as per tensorflow2 guidelines\n",
        "\n",
        "model = MyModel()\n",
        "\n",
        "loss_obj = tf.keras.losses.MeanSquaredError()\n",
        "optimizer = tf.keras.optimizers.Adam( learning_rate=0.00005)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.MeanSquaredError(name='train_accuracy')\n",
        "\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.MeanSquaredError(name='test_accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLAS1UIrC2FV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## group data by query\n",
        "\n",
        "groupedDataset = df.groupby('query')\n",
        "queries = []\n",
        "for q,g in groupedDataset:\n",
        "  queries.append(q)\n",
        "\n",
        "queries = shuffle(queries)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6VgRuICPLSm",
        "colab_type": "code",
        "outputId": "dc54ea5d-2efd-4639-ae57-31b06696e570",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "\n",
        "data_length = len(groupedDataset)\n",
        "split_length = int(0.8*data_length)\n",
        "\n",
        "queries_for_training = queries[:split_length]\n",
        "queries_for_testing = queries[split_length:]\n",
        "\n",
        "test_data =[]\n",
        "count = 0\n",
        "\n",
        "queryTrainList = []\n",
        "predTrainList = []\n",
        "objTrainList = []\n",
        "\n",
        "y_train = []\n",
        "\n",
        "for query,group in groupedDataset:\n",
        "  if(query in queries_for_testing):\n",
        "    test_data.append([query,group[\"pred\"].values,group[\"obj\"].values,group[\"rel\"].values])\n",
        "  else: \n",
        "    for i in range(len(group[\"pred\"].values)):\n",
        "      queryTrainList.append(query)\n",
        "      predTrainList.append(group[\"pred\"].values[i])\n",
        "      objTrainList.append(group[\"obj\"].values[i])\n",
        "      y_train.append(group[\"rel\"].values[i])\n",
        "  count+=1\n",
        "\n",
        "## To give a higher importance to a higher utility value\n",
        "def invertRanks(rankArray):\n",
        "  maximum = 2 \n",
        "  for i in range(len(rankArray)):\n",
        "    rankArray[i]=rankArray[i]/maximum\n",
        "  return rankArray\n",
        "\n",
        "\n",
        "y_train = invertRanks(y_train)\n",
        "\n",
        "\n",
        "\n",
        "queryTrainList,predTrainList,objTrainList,y_train = shuffle(queryTrainList,predTrainList,objTrainList,y_train)\n",
        "\n",
        "\n",
        "print(y_train[0:10])    \n",
        "print(len(queryTrainList),len(predTrainList),len(objTrainList),len(test_data),len(y_train))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.0, 0.5, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "997 997 997 19 997\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNOJib6ZR4ZP",
        "colab_type": "code",
        "outputId": "f15edfc1-a40d-4824-c06f-8572417d145a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(944/16\n",
        "      )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "59.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tQvFmHdRRjs",
        "colab_type": "code",
        "outputId": "9388487e-d9a2-4d6b-d2ba-e010e7401d22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "batch_size = 16\n",
        "num_batches = math.floor(len(queryTrainList)/batch_size)\n",
        "EPOCHS = 10\n",
        "\n",
        "bert_layer = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "\n",
        "def train_step(query,pred,obj,target):\n",
        "    input_ids = []\n",
        "    attn_mask = []\n",
        "    segment_ids = []\n",
        "\n",
        "    for k in range(len(query)):\n",
        "          in_id = tokenizer.encode(query[k],pred[k]+\" \"+obj[k], add_special_tokens=True)  # Batch size 1\n",
        "          # print(query[k])\n",
        "          # print(pred[k])\n",
        "          # print(obj[k])\n",
        "          # print(in_id)\n",
        "          # input()\n",
        "          attn,seg = getBertParameters(tokenizer.convert_ids_to_tokens(in_id))\n",
        "          in_id += [0]*(maxLengthPadding-len(in_id))\n",
        "          input_ids.append(in_id)\n",
        "          attn_mask.append(attn)\n",
        "          segment_ids.append(seg)\n",
        "          \n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        input_dictionary['input_ids'] = tf.convert_to_tensor(np.array(input_ids)) ## [None, :]\n",
        "        input_dictionary['attention_mask'] = tf.convert_to_tensor(np.array(attn_mask))  ## [None, :]\n",
        "        input_dictionary['token_type_ids'] = tf.convert_to_tensor(np.array(segment_ids))  ## [None, :]\n",
        "        output = bert_layer(input_dictionary)\n",
        "        output = model(output)\n",
        "        loss = (target-output)**2\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    \n",
        "    return loss\n",
        "  \n",
        "        \n",
        "    \n",
        "\n",
        "## [CLS] + [Query] + [SEP] + [PRED + OBJ] +[SEP]\n",
        "for j in range(EPOCHS):\n",
        "    for i in range(num_batches-1):\n",
        "        index = i*batch_size\n",
        "        q = queryTrainList[index:index+batch_size]\n",
        "        p = predTrainList[index:index+batch_size]\n",
        "        o = objTrainList[index:index+batch_size]\n",
        "        actual_y = np.array(y_train[index:index+batch_size]).reshape(batch_size,1)\n",
        "        loss = train_step(q,p,o,actual_y)\n",
        "        q = queryTrainList[num_batches*batch_size:]\n",
        "        p = predTrainList[num_batches*batch_size:]\n",
        "        o = objTrainList[num_batches*batch_size:]\n",
        "        actual_y = y_train[num_batches*batch_size:]\n",
        "        actual_y = np.array(actual_y).reshape(len(actual_y),1)\n",
        "    if(len(q)>1):  \n",
        "    \n",
        "        loss = train_step(q,p,o,actual_y)\n",
        "    # print(\"Loss after epoch \"+str(j)+\" :\", loss)   \n",
        "    n5,n10 = test_step()\n",
        "    print(n5,n10)\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4110386160510052 0.5083036143547622\n",
            "0.41434326528693805 0.5208195199305636\n",
            "0.45730443876928445 0.5513072666492504\n",
            "0.4459550088033007 0.5482176575886468\n",
            "0.4647234966589734 0.5608954150372455\n",
            "0.4616105626330766 0.5585798823232837\n",
            "0.46392498349881445 0.5631546265299179\n",
            "0.45908369695788076 0.5584295364896438\n",
            "0.46543579417984726 0.5628844836195648\n",
            "0.4710748919242081 0.5655042989393879\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3yk5maAYIs-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Bert w/o fine tuning on rel for URI data\n",
        "\n",
        "0.48075428383371943 0.5528401726926597\n",
        "0.48657867716690933 0.5946955715174369\n",
        "0.4166653936829592 0.5105340440227891\n",
        "0.436182790122136 0.5328894087396685\n",
        "0.4616105626330766 0.5585798823232837\n",
        "\n",
        "\n",
        "# avg :   .4563      .5498"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETUejsMdbRpX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# model_2.save_weights('/content/drive/My Drive/Independent Study/MiniBatch_URI_Data')\n",
        "# # model_2.save_weights('/content/drive/My Drive/Independent Study/MiniBatch_Entire_Data_Imp')\n",
        "# model_load = MyModel2(vocab_size)\n",
        "# model_load.load_weights('/content/drive/My Drive/Independent Study/MiniBatch_URI_Data')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rf6hgr1BA_Aq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # ## grouping the dataset by query to get the ndcg scores\n",
        "def test_step():\n",
        "    count = 0\n",
        "    integerValuedQueryRanks = []\n",
        "    groundTruthRanks = []\n",
        "    for x_test in test_data:\n",
        "        query = x_test[0]\n",
        "        predPerQuery = list(x_test[1])\n",
        "        objPerQuery = list(x_test[2])\n",
        "        target = list(x_test[3])\n",
        "        g_t = []\n",
        "        i_ranks = []\n",
        "        predictedRanksPerQuery = []\n",
        "        for i in range(len(predPerQuery)):\n",
        "            input_ids = tokenizer.encode(query,predPerQuery[i]+\" \"+objPerQuery[i], add_special_tokens=True)  # Batch size 1\n",
        "            attn_mask,segment_id = getBertParameters(tokenizer.convert_ids_to_tokens(input_ids))\n",
        "            input_ids += [0]*(maxLengthPadding-len(input_ids))\n",
        "            input_dictionary['input_ids'] = tf.constant(input_ids)[None, :]\n",
        "            input_dictionary['attention_mask'] = tf.constant(attn_mask)[None, :]\n",
        "            input_dictionary['token_type_ids'] = tf.constant(segment_id)[None, :]\n",
        "            output = bert_layer(input_dictionary)\n",
        "            output = model(output)\n",
        "            # predictedRanksPerQuery.append(output.numpy().tolist()[0][0])\n",
        "            i_ranks.append(round(output.numpy().tolist()[0][0]*2))\n",
        "        integerValuedQueryRanks.append(i_ranks)\n",
        "\n",
        "        groundTruthRanks.append(target) \n",
        "        # predictedQueryRanks.append(predictedRanksPerQuery)\n",
        "        count+=1\n",
        "    return ndcg_scores(groundTruthRanks,integerValuedQueryRanks)\n",
        "\n",
        "# for x,y in zip(groundTruthRanks,integerValuedQueryRanks):\n",
        "#     print(\"GRound truth : \", x)\n",
        "#     print(\"Predicted : \" ,y)\n",
        "#     print(\"----------------------------------------------------------x-----------------------------------------x---------------------------\")\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZRibhmBBAJN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from sklearn.metrics import ndcg_score, dcg_score \n",
        "def ndcg_scores(groundTruthRanks,integerValuedQueryRanks):  \n",
        "  ndcg_scores_5 = []\n",
        "  ndcg_scores_10 = []\n",
        "  count = 0\n",
        "  for x,y in zip(groundTruthRanks,integerValuedQueryRanks):\n",
        "      \n",
        "      \n",
        "      if(len(x)>1):\n",
        "          true_relevance = np.asarray([x]) \n",
        "          relevance_score = np.asarray([y]) \n",
        "\n",
        "          ndcg_scores_5.append(ndcg_score(true_relevance, relevance_score,k=5))\n",
        "          ndcg_scores_10.append(ndcg_score(true_relevance, relevance_score,k=10))\n",
        "  \n",
        "\n",
        "  n5 = sum(ndcg_scores_5)/len(ndcg_scores_5)\n",
        "  n10 = sum(ndcg_scores_10)/len(ndcg_scores_10)\n",
        "\n",
        "  return n5,n10\n",
        "\n",
        "   \n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gq9KwC8QBOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NDCG@5: 0.7981107576995108\n",
        "# NDCG@10: 0.8366214915585496\n",
        "\n",
        "# for whithout fine tuning bert - partial dataset train test split\n",
        "\n",
        "# NDCG@5: 0.6234769111104137\n",
        "# NDCG@10: 0.6933181533971675\n",
        "\n",
        "# for whithout fine tuning bert - complete dataset train test split\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgOcn99eaS-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
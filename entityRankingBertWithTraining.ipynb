{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "entityRankingBertWithTraining.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "slCrtu6DptEj",
        "colab_type": "code",
        "outputId": "294c5de8-dbed-4158-a22c-f03c19bd2cc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install --upgrade tensorflow\n",
        "!pip install transformers\n",
        "!pip install keras\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "# !pip install -q pyyaml h5py\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import keras\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import math\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# from keras.layers import LSTM, GRU\n",
        "# from keras.layers.embeddings import Embedding\n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "# from keras.models import Model\n",
        "from keras.layers import Input, Dense, Bidirectional,Flatten\n",
        "# from keras.layers import concatenate,Dropout\n",
        "from transformers import BertTokenizer, TFBertModel,TFBertForSequenceClassification"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorflow in /usr/local/lib/python3.6/dist-packages (2.2.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.4)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.2.1)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.28.1)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (46.1.3)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.6.0.post3)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.7.2)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.9)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2020.4.5.1)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.0)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.8)\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/97/7db72a0beef1825f82188a4b923e62a146271ac2ced7928baa4d47ef2467/transformers-2.9.1-py3-none-any.whl (641kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 9.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 54.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 45.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/88/49e772d686088e1278766ad68a463513642a2a877487decbd691dec02955/sentencepiece-0.1.90-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 47.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=9d102632d7051b485376ef866473cdebd30b8c4c84abb23cc6d08e47ce8b7530\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.90 tokenizers-0.7.0 transformers-2.9.1\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.3.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.4)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qm5Pw8lRrdF_",
        "colab_type": "code",
        "outputId": "9b539602-c39d-4ea9-c125-94decf609487",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14DZjA9qq2P_",
        "colab_type": "text"
      },
      "source": [
        "Given a natural language query $q$ and a set of (predicate, object) tuples pertaining to an entity, the model aims to predict the *importance* and *utility* ranks of the (predicate, object) tuples with respect to the query $q$.\n",
        "\n",
        "We encode the query $q$ and the predicate $p$ into $d$-dimensional embedding vectors $h_q$ and $h_p$ using a BiLSTM. Then, the model predicts a score for by using a dot product of $h_q$ and $h_p$. i.e. $\\Psi(q, p) = h_q^Th_p$\n",
        "\n",
        "We minimize the MSE Loss between the predicted score and the target utility or importance score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-48b_J8Djeo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.bert_layer = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dense_layer = tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "        ## 50 x 768\n",
        "\n",
        "    def __call__(self, x_train):\n",
        "        intermediate = self.bert_layer(x_train)\n",
        "        output = self.dense_layer(intermediate[1])\n",
        "        \n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fitav0Tu1ot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## homogeneous encoding for all the queries,predicates, objects\n",
        "maxLengthPadding = 50\n",
        "# Note : maxLengthOfTokens = 48;\n",
        "\n",
        "input_dictionary = {}\n",
        "\n",
        "# df = pd.read_csv(\"/content/drive/My Drive/Independent Study/Modified_Data.csv\")\n",
        "# df = pd.read_csv(\"/content/drive/My Drive/Independent Study/URI_only.csv\")\n",
        "# df = pd.read_csv(\"/content/drive/My Drive/Independent Study/Complete_Data_With_Targets.csv\")\n",
        "# df = pd.read_csv(\"/content/drive/My Drive/Independent Study/URI_with_imp.csv\")\n",
        "df = pd.read_csv(\"/content/drive/My Drive/Independent Study/URI_with_targets.csv\")\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def getBertParameters(tokens):\n",
        "    attn_mask = []\n",
        "    seg_ids = []\n",
        "    # pos_ids = []\n",
        "    if(len(tokens)<maxLengthPadding):\n",
        "        attn_mask = [1]*len(tokens) + [0]*(maxLengthPadding-len(tokens))\n",
        "    else:\n",
        "        attn_mask = [1]*maxLengthPadding\n",
        "    \n",
        "    segment = 0\n",
        "    for x in tokens:\n",
        "        seg_ids.append(segment)\n",
        "        if(x=='[SEP]'):\n",
        "            segment = 1\n",
        "    seg_ids+=[0]*(maxLengthPadding-len(tokens))\n",
        "    return attn_mask,seg_ids\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Py8T5xBrV93V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## defining custom model parameters as per tensorflow2 guidelines\n",
        "\n",
        "model = MyModel()\n",
        "\n",
        "loss_obj = tf.keras.losses.MeanSquaredError()\n",
        "optimizer = tf.keras.optimizers.Adam( learning_rate=0.00005)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.MeanSquaredError(name='train_accuracy')\n",
        "\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.MeanSquaredError(name='test_accuracy')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CnqCSfmZehT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "groupedDataset = df.groupby('query')\n",
        "queries = []\n",
        "for q,g in groupedDataset:\n",
        "  queries.append(q)\n",
        "\n",
        "queries = shuffle(queries)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6VgRuICPLSm",
        "colab_type": "code",
        "outputId": "8a420516-3524-496d-c296-24a339242842",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "## data length = 95\n",
        "data_length = len(groupedDataset)\n",
        "split_length = int(0.8*data_length)\n",
        "\n",
        "queries_for_training = queries[:split_length]\n",
        "queries_for_testing = queries[split_length:]\n",
        "\n",
        "\n",
        "test_data =[]\n",
        "count = 0\n",
        "\n",
        "queryTrainList = []\n",
        "predTrainList = []\n",
        "objTrainList = []\n",
        "\n",
        "y_train = []\n",
        "\n",
        "for query,group in groupedDataset:\n",
        "    if(query in queries_for_testing):\n",
        "      test_data.append([query,group[\"pred\"].values,group[\"obj\"].values,group[\"imp\"].values])\n",
        "    else: \n",
        "      for i in range(len(group[\"pred\"].values)):\n",
        "        queryTrainList.append(query)\n",
        "        predTrainList.append(group[\"pred\"].values[i])\n",
        "        objTrainList.append(group[\"obj\"].values[i])\n",
        "        y_train.append(group[\"imp\"].values[i])\n",
        "    count+=1\n",
        "\n",
        "## To give a higher importance to a higher utility value\n",
        "def normalizeRanks(rankArray):\n",
        "  maximum = 2\n",
        "  for i in range(len(rankArray)):\n",
        "    rankArray[i]=rankArray[i]/maximum\n",
        "  return rankArray\n",
        "\n",
        "\n",
        "y_train = normalizeRanks(y_train)\n",
        "\n",
        "\n",
        "\n",
        "queryTrainList,predTrainList,objTrainList,y_train = shuffle(queryTrainList,predTrainList,objTrainList,y_train)\n",
        "\n",
        "\n",
        "print(y_train[0:10])    \n",
        "print(len(queryTrainList),len(predTrainList),len(objTrainList),len(test_data),len(y_train),data_length)\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.0, 1.0, 0.5, 0.0, 0.5, 0.5, 0.5, 0.5, 1.0, 0.5]\n",
            "1097 1097 1097 19 1097 95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tQvFmHdRRjs",
        "colab_type": "code",
        "outputId": "d2c2d8c9-1388-4854-e342-a31a015eb1dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "batch_size = 16\n",
        "num_batches = math.floor(len(queryTrainList)/batch_size)\n",
        "EPOCHS = 20\n",
        "\n",
        "\n",
        "def train_step(query,pred,obj,target):\n",
        "    input_ids = []\n",
        "    attn_mask = []\n",
        "    segment_ids = []\n",
        "\n",
        "    for k in range(len(query)):\n",
        "          in_id = tokenizer.encode(query[k],pred[k]+\" \"+obj[k], add_special_tokens=True)  # Batch size 1\n",
        "          attn,seg = getBertParameters(tokenizer.convert_ids_to_tokens(in_id))\n",
        "          in_id += [0]*(maxLengthPadding-len(in_id))\n",
        "          input_ids.append(in_id)\n",
        "          attn_mask.append(attn)\n",
        "          segment_ids.append(seg)\n",
        "          \n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        input_dictionary['input_ids'] = tf.convert_to_tensor(np.array(input_ids)) ## [None, :]\n",
        "        input_dictionary['attention_mask'] = tf.convert_to_tensor(np.array(attn_mask))  ## [None, :]\n",
        "        input_dictionary['token_type_ids'] = tf.convert_to_tensor(np.array(segment_ids))  ## [None, :]\n",
        "        output = model(input_dictionary)\n",
        "        loss = (target-output)**2\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "        # input()\n",
        "    return loss\n",
        "  \n",
        "        \n",
        "    \n",
        "ndcg_epoch_array = {\n",
        "    \n",
        "}\n",
        "\n",
        "## [CLS] + [Query] + [SEP] + [PRED + OBJ] +[SEP]\n",
        "for j in range(EPOCHS):\n",
        "    for i in range(num_batches-1):\n",
        "        index = i*batch_size\n",
        "        q = queryTrainList[index:index+batch_size]\n",
        "        p = predTrainList[index:index+batch_size]\n",
        "        o = objTrainList[index:index+batch_size]\n",
        "        actual_y = np.array(y_train[index:index+batch_size]).reshape(batch_size,1)\n",
        "        loss = train_step(q,p,o,actual_y)\n",
        "\n",
        "    q = queryTrainList[num_batches*batch_size:]\n",
        "    p = predTrainList[num_batches*batch_size:]\n",
        "    o = objTrainList[num_batches*batch_size:]\n",
        "    if(len(q)>0):\n",
        "        actual_y = y_train[num_batches*batch_size:]\n",
        "        actual_y = np.array(actual_y).reshape(len(actual_y),1)\n",
        "        print(actual_y)\n",
        "        \n",
        "        loss = train_step(q,p,o,actual_y)\n",
        "        print(\"Loss after epoch \"+str(j)+\" :\", loss)   \n",
        "    n5,n10 = test_step()\n",
        "\n",
        "    print(\"n5,n10:\",n5,n10)\n",
        "\n",
        "\n",
        "    ndcg_epoch_array[j] = [n5,n10]\n",
        "\n",
        "\n",
        "\n",
        "    \n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. ]\n",
            " [0.5]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0.5]\n",
            " [1. ]\n",
            " [0.5]]\n",
            "Loss after epoch 0 : tf.Tensor(\n",
            "[[0.01046777]\n",
            " [0.0062552 ]\n",
            " [0.03520949]\n",
            " [0.30581668]\n",
            " [0.03121926]\n",
            " [0.01727354]\n",
            " [0.05679343]\n",
            " [0.00218958]\n",
            " [0.02441305]], shape=(9, 1), dtype=float32)\n",
            "[2, 0, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2]\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "n5,n10: 0.8277502737893991 0.8539832714891658\n",
            "[[0. ]\n",
            " [0.5]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0.5]\n",
            " [1. ]\n",
            " [0.5]]\n",
            "Loss after epoch 1 : tf.Tensor(\n",
            "[[6.8226797e-03]\n",
            " [1.0796040e-03]\n",
            " [1.9329825e-03]\n",
            " [3.4071097e-01]\n",
            " [2.4150020e-02]\n",
            " [3.0934403e-02]\n",
            " [1.3142760e-01]\n",
            " [2.9792663e-04]\n",
            " [2.6681462e-02]], shape=(9, 1), dtype=float32)\n",
            "[2, 0, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2]\n",
            "[1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2]\n",
            "n5,n10: 0.87788574494226 0.8981272203961784\n",
            "[[0. ]\n",
            " [0.5]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0.5]\n",
            " [1. ]\n",
            " [0.5]]\n",
            "Loss after epoch 2 : tf.Tensor(\n",
            "[[1.5695360e-03]\n",
            " [1.9263295e-03]\n",
            " [7.1019805e-03]\n",
            " [1.4125182e-01]\n",
            " [1.5240103e-01]\n",
            " [2.3279499e-02]\n",
            " [1.6105184e-01]\n",
            " [1.4658990e-04]\n",
            " [8.5850367e-03]], shape=(9, 1), dtype=float32)\n",
            "[2, 0, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2]\n",
            "[2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2]\n",
            "n5,n10: 0.889085452066925 0.9055034774742943\n",
            "[[0. ]\n",
            " [0.5]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0.5]\n",
            " [1. ]\n",
            " [0.5]]\n",
            "Loss after epoch 3 : tf.Tensor(\n",
            "[[1.21891703e-02]\n",
            " [7.14527967e-04]\n",
            " [8.59647291e-04]\n",
            " [1.54650703e-01]\n",
            " [1.77944414e-02]\n",
            " [1.52067905e-02]\n",
            " [1.89863518e-01]\n",
            " [1.23047168e-04]\n",
            " [4.42788936e-03]], shape=(9, 1), dtype=float32)\n",
            "[2, 0, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2]\n",
            "[2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2]\n",
            "n5,n10: 0.8779874770667968 0.9048284050797816\n",
            "[[0. ]\n",
            " [0.5]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0.5]\n",
            " [1. ]\n",
            " [0.5]]\n",
            "Loss after epoch 4 : tf.Tensor(\n",
            "[[7.72176892e-04]\n",
            " [1.06454587e-04]\n",
            " [1.45680329e-03]\n",
            " [1.39101624e-01]\n",
            " [1.24218855e-02]\n",
            " [2.03216001e-02]\n",
            " [2.15752348e-01]\n",
            " [4.84105549e-05]\n",
            " [7.68433325e-04]], shape=(9, 1), dtype=float32)\n",
            "[2, 0, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2]\n",
            "[2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2]\n",
            "n5,n10: 0.8717839073504172 0.8979630805328477\n",
            "[[0. ]\n",
            " [0.5]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0.5]\n",
            " [1. ]\n",
            " [0.5]]\n",
            "Loss after epoch 5 : tf.Tensor(\n",
            "[[2.7280502e-04]\n",
            " [1.2246555e-05]\n",
            " [6.1477214e-05]\n",
            " [6.6258311e-02]\n",
            " [2.0788930e-04]\n",
            " [8.6949971e-03]\n",
            " [2.2960094e-01]\n",
            " [1.7089458e-05]\n",
            " [3.7563758e-03]], shape=(9, 1), dtype=float32)\n",
            "[2, 0, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2]\n",
            "[2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2]\n",
            "n5,n10: 0.8566280681212103 0.8832700771987593\n",
            "[[0. ]\n",
            " [0.5]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0.5]\n",
            " [1. ]\n",
            " [0.5]]\n",
            "Loss after epoch 6 : tf.Tensor(\n",
            "[[2.1328300e-03]\n",
            " [1.5475198e-03]\n",
            " [8.6436958e-05]\n",
            " [2.6272530e-02]\n",
            " [1.9348131e-03]\n",
            " [1.0098383e-03]\n",
            " [2.3908979e-01]\n",
            " [1.4550550e-05]\n",
            " [3.5864518e-03]], shape=(9, 1), dtype=float32)\n",
            "[2, 0, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2]\n",
            "[2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 1, 2]\n",
            "n5,n10: 0.8390781753326506 0.8748321524517114\n",
            "[[0. ]\n",
            " [0.5]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0.5]\n",
            " [1. ]\n",
            " [0.5]]\n",
            "Loss after epoch 7 : tf.Tensor(\n",
            "[[2.51914345e-04]\n",
            " [1.24176918e-02]\n",
            " [1.06565142e-03]\n",
            " [2.92389572e-01]\n",
            " [9.30559337e-02]\n",
            " [3.02538127e-02]\n",
            " [2.36161426e-01]\n",
            " [1.49428597e-05]\n",
            " [1.38352895e-02]], shape=(9, 1), dtype=float32)\n",
            "[2, 0, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2]\n",
            "[2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2]\n",
            "n5,n10: 0.8664522257853488 0.8944954444811369\n",
            "[[0. ]\n",
            " [0.5]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0.5]\n",
            " [1. ]\n",
            " [0.5]]\n",
            "Loss after epoch 8 : tf.Tensor(\n",
            "[[1.67179707e-04]\n",
            " [1.45398844e-02]\n",
            " [6.57007564e-04]\n",
            " [1.81805447e-01]\n",
            " [7.73951004e-04]\n",
            " [3.56891600e-04]\n",
            " [2.36269936e-01]\n",
            " [1.07731585e-05]\n",
            " [1.25722643e-02]], shape=(9, 1), dtype=float32)\n",
            "[2, 0, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2]\n",
            "[2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2]\n",
            "n5,n10: 0.8623078107890673 0.8898963963758205\n",
            "[[0. ]\n",
            " [0.5]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0.5]\n",
            " [1. ]\n",
            " [0.5]]\n",
            "Loss after epoch 9 : tf.Tensor(\n",
            "[[8.9384295e-05]\n",
            " [1.3315957e-03]\n",
            " [2.0067064e-05]\n",
            " [2.4636842e-02]\n",
            " [6.1671576e-03]\n",
            " [6.2820932e-04]\n",
            " [2.2858234e-01]\n",
            " [7.4149370e-06]\n",
            " [1.7321464e-03]], shape=(9, 1), dtype=float32)\n",
            "[2, 0, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2]\n",
            "[2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2]\n",
            "n5,n10: 0.8935991874173155 0.9092718528937019\n",
            "[[0. ]\n",
            " [0.5]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0.5]\n",
            " [1. ]\n",
            " [0.5]]\n",
            "Loss after epoch 10 : tf.Tensor(\n",
            "[[3.9812716e-05]\n",
            " [4.3504057e-05]\n",
            " [1.8965491e-05]\n",
            " [1.7713697e-03]\n",
            " [3.6698654e-03]\n",
            " [3.8841731e-04]\n",
            " [2.1245101e-01]\n",
            " [9.1052161e-06]\n",
            " [5.4667944e-05]], shape=(9, 1), dtype=float32)\n",
            "[2, 0, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2]\n",
            "[2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2]\n",
            "n5,n10: 0.8592639955825111 0.8830790777186778\n",
            "[[0. ]\n",
            " [0.5]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0.5]\n",
            " [1. ]\n",
            " [0.5]]\n",
            "Loss after epoch 11 : tf.Tensor(\n",
            "[[1.2323109e-04]\n",
            " [1.0942498e-03]\n",
            " [7.1977774e-05]\n",
            " [2.0046378e-03]\n",
            " [7.3635322e-03]\n",
            " [1.2000330e-03]\n",
            " [1.8824692e-01]\n",
            " [5.9136992e-06]\n",
            " [5.0408143e-04]], shape=(9, 1), dtype=float32)\n",
            "[2, 0, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2]\n",
            "[2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2]\n",
            "n5,n10: 0.8690262393892575 0.8897577142256277\n",
            "[[0. ]\n",
            " [0.5]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0.5]\n",
            " [1. ]\n",
            " [0.5]]\n",
            "Loss after epoch 12 : tf.Tensor(\n",
            "[[4.6123700e-05]\n",
            " [3.9166273e-04]\n",
            " [2.6435609e-05]\n",
            " [4.8429929e-04]\n",
            " [1.0331606e-04]\n",
            " [1.3529459e-03]\n",
            " [1.7642300e-01]\n",
            " [5.0373892e-06]\n",
            " [2.3568659e-03]], shape=(9, 1), dtype=float32)\n",
            "[2, 0, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2]\n",
            "[2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2]\n",
            "n5,n10: 0.860806547814115 0.8873771193762889\n",
            "[[0. ]\n",
            " [0.5]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0.5]\n",
            " [1. ]\n",
            " [0.5]]\n",
            "Loss after epoch 13 : tf.Tensor(\n",
            "[[3.42616149e-05]\n",
            " [2.94737674e-06]\n",
            " [3.50784430e-05]\n",
            " [4.29677602e-05]\n",
            " [1.10084584e-04]\n",
            " [8.30911886e-05]\n",
            " [1.22710809e-01]\n",
            " [6.12485474e-06]\n",
            " [3.81072095e-05]], shape=(9, 1), dtype=float32)\n",
            "[2, 0, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2]\n",
            "[2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2]\n",
            "n5,n10: 0.8553308714215051 0.8771429845741393\n",
            "[[0. ]\n",
            " [0.5]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0.5]\n",
            " [1. ]\n",
            " [0.5]]\n",
            "Loss after epoch 14 : tf.Tensor(\n",
            "[[1.67961734e-05]\n",
            " [4.14965063e-04]\n",
            " [1.59549363e-05]\n",
            " [1.37240073e-04]\n",
            " [5.58266220e-05]\n",
            " [2.66738662e-05]\n",
            " [1.24692574e-01]\n",
            " [5.12849829e-06]\n",
            " [1.67234364e-04]], shape=(9, 1), dtype=float32)\n",
            "[2, 0, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2]\n",
            "[1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2]\n",
            "n5,n10: 0.8419030133663847 0.8684671905589285\n",
            "[[0. ]\n",
            " [0.5]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0.5]\n",
            " [1. ]\n",
            " [0.5]]\n",
            "Loss after epoch 15 : tf.Tensor(\n",
            "[[1.0769342e-05]\n",
            " [3.3823814e-04]\n",
            " [8.6548034e-06]\n",
            " [7.6718621e-05]\n",
            " [5.1029238e-05]\n",
            " [3.4448698e-05]\n",
            " [1.2703389e-01]\n",
            " [3.3449080e-06]\n",
            " [1.1147539e-03]], shape=(9, 1), dtype=float32)\n",
            "[2, 0, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2]\n",
            "[2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2]\n",
            "n5,n10: 0.8706392398884882 0.89613450349768\n",
            "[[0. ]\n",
            " [0.5]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0.5]\n",
            " [1. ]\n",
            " [0.5]]\n",
            "Loss after epoch 16 : tf.Tensor(\n",
            "[[1.14395125e-05]\n",
            " [5.27301570e-04]\n",
            " [7.48718185e-06]\n",
            " [5.90396463e-04]\n",
            " [3.09514144e-05]\n",
            " [2.37309341e-05]\n",
            " [1.79239400e-02]\n",
            " [4.28617432e-06]\n",
            " [1.46460749e-04]], shape=(9, 1), dtype=float32)\n",
            "[2, 0, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2]\n",
            "[2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1]\n",
            "n5,n10: 0.8802355114169 0.8942004165882376\n",
            "[[0. ]\n",
            " [0.5]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0.5]\n",
            " [1. ]\n",
            " [0.5]]\n",
            "Loss after epoch 17 : tf.Tensor(\n",
            "[[4.9133050e-06]\n",
            " [3.9655436e-04]\n",
            " [3.2044713e-06]\n",
            " [1.5996188e-04]\n",
            " [1.3200340e-05]\n",
            " [6.1494618e-04]\n",
            " [7.7367004e-04]\n",
            " [1.9866086e-06]\n",
            " [2.2855760e-04]], shape=(9, 1), dtype=float32)\n",
            "[2, 0, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2]\n",
            "[2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2]\n",
            "n5,n10: 0.8769840838701812 0.8894277490431457\n",
            "[[0. ]\n",
            " [0.5]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0.5]\n",
            " [1. ]\n",
            " [0.5]]\n",
            "Loss after epoch 18 : tf.Tensor(\n",
            "[[1.1478842e-05]\n",
            " [2.8628943e-04]\n",
            " [2.6015655e-04]\n",
            " [2.6052941e-03]\n",
            " [1.1190236e-04]\n",
            " [2.0648347e-05]\n",
            " [6.1739767e-03]\n",
            " [1.1416749e-05]\n",
            " [1.7419476e-04]], shape=(9, 1), dtype=float32)\n",
            "[2, 0, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2]\n",
            "[2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2]\n",
            "n5,n10: 0.8383861263361176 0.8615554968996788\n",
            "[[0. ]\n",
            " [0.5]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0. ]\n",
            " [0.5]\n",
            " [1. ]\n",
            " [0.5]]\n",
            "Loss after epoch 19 : tf.Tensor(\n",
            "[[4.4149732e-05]\n",
            " [2.9029425e-03]\n",
            " [3.9481180e-05]\n",
            " [5.4963391e-05]\n",
            " [5.6446806e-05]\n",
            " [3.1413283e-04]\n",
            " [5.7949196e-03]\n",
            " [5.5271353e-06]\n",
            " [4.8535797e-03]], shape=(9, 1), dtype=float32)\n",
            "[2, 0, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2]\n",
            "[2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2]\n",
            "n5,n10: 0.8478862344999464 0.8699431071753785\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h90ez6Z1htcz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "168deca8-a35d-4d1f-a0e7-1820a576da93"
      },
      "source": [
        "ndcg_epoch_array\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: [0.8277502737893991, 0.8539832714891658],\n",
              " 1: [0.87788574494226, 0.8981272203961784],\n",
              " 2: [0.889085452066925, 0.9055034774742943],\n",
              " 3: [0.8779874770667968, 0.9048284050797816],\n",
              " 4: [0.8717839073504172, 0.8979630805328477],\n",
              " 5: [0.8566280681212103, 0.8832700771987593],\n",
              " 6: [0.8390781753326506, 0.8748321524517114],\n",
              " 7: [0.8664522257853488, 0.8944954444811369],\n",
              " 8: [0.8623078107890673, 0.8898963963758205],\n",
              " 9: [0.8935991874173155, 0.9092718528937019],\n",
              " 10: [0.8592639955825111, 0.8830790777186778],\n",
              " 11: [0.8690262393892575, 0.8897577142256277],\n",
              " 12: [0.860806547814115, 0.8873771193762889],\n",
              " 13: [0.8553308714215051, 0.8771429845741393],\n",
              " 14: [0.8419030133663847, 0.8684671905589285],\n",
              " 15: [0.8706392398884882, 0.89613450349768],\n",
              " 16: [0.8802355114169, 0.8942004165882376],\n",
              " 17: [0.8769840838701812, 0.8894277490431457],\n",
              " 18: [0.8383861263361176, 0.8615554968996788],\n",
              " 19: [0.8478862344999464, 0.8699431071753785]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW5-9wsnm_7M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ## grouping the dataset by query to get the ndcg scores\n",
        "def test_step():\n",
        "    predictedQueryRanks = []\n",
        "    groundTruthRanks = []\n",
        "\n",
        "    integerValuedQueryRanks = []\n",
        "\n",
        "    count = 0\n",
        "\n",
        "    for x_test in test_data:\n",
        "        query = x_test[0]\n",
        "        predPerQuery = list(x_test[1])\n",
        "        objPerQuery = list(x_test[2])\n",
        "        target = list(x_test[3])\n",
        "        g_t = []\n",
        "        i_ranks = []\n",
        "        predictedRanksPerQuery = []\n",
        "        for i in range(len(predPerQuery)):\n",
        "            input_ids = tokenizer.encode(query,predPerQuery[i]+\" \"+objPerQuery[i], add_special_tokens=True)  # Batch size 1\n",
        "            attn_mask,segment_id = getBertParameters(tokenizer.convert_ids_to_tokens(input_ids))\n",
        "            input_ids += [0]*(maxLengthPadding-len(input_ids))\n",
        "            input_dictionary['input_ids'] = tf.constant(input_ids)[None, :]\n",
        "            input_dictionary['attention_mask'] = tf.constant(attn_mask)[None, :]\n",
        "            input_dictionary['token_type_ids'] = tf.constant(segment_id)[None, :]\n",
        "            output = model(input_dictionary)\n",
        "            predictedRanksPerQuery.append(output.numpy().tolist()[0][0])\n",
        "            i_ranks.append(round(output.numpy().tolist()[0][0]*2))\n",
        "        integerValuedQueryRanks.append(i_ranks)\n",
        "\n",
        "        groundTruthRanks.append(target) \n",
        "        predictedQueryRanks.append(predictedRanksPerQuery)\n",
        "        count+=1\n",
        "    print(groundTruthRanks[0])\n",
        "    print(integerValuedQueryRanks[0])\n",
        "    \n",
        "    return compute_ndcg_scores(groundTruthRanks,integerValuedQueryRanks)\n",
        "\n",
        "    # print(groundTruthRanks[0])\n",
        "    # print(predictedQueryRanks[0])\n",
        "    # print(integerValuedQueryRanks[0])\n",
        "\n",
        "    # for x,y in zip(groundTruthRanks,integerValuedQueryRanks):\n",
        "    #     print(\"GRound truth : \", x)\n",
        "    #     print(\"Predicted : \" ,y)\n",
        "    #     print(\"----------------------------------------------------------x-----------------------------------------x---------------------------\")\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WINuywE98VMv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from sklearn.metrics import ndcg_score, dcg_score \n",
        "\n",
        "def compute_ndcg_scores(groundTruthRanks,integerValuedQueryRanks): \n",
        "    ndcg_scores_5 = []\n",
        "    ndcg_scores_10 = []\n",
        "    count = 0\n",
        "    for x,y in zip(groundTruthRanks,integerValuedQueryRanks):\n",
        "        \n",
        "        \n",
        "        if(len(x)>1):\n",
        "            true_relevance = np.asarray([x]) \n",
        "            relevance_score = np.asarray([y]) \n",
        "\n",
        "            ndcg_scores_5.append(ndcg_score(true_relevance, relevance_score,k=5))\n",
        "            ndcg_scores_10.append(ndcg_score(true_relevance, relevance_score,k=10))\n",
        "        \n",
        "\n",
        "    return (sum(ndcg_scores_5)/len(ndcg_scores_5)),(sum(ndcg_scores_10)/len(ndcg_scores_10))\n",
        "\n",
        "\n",
        "\n",
        "   \n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgOcn99eaS-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}